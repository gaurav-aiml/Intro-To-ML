{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training using Neural Networks from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import util_mnist_reader\n",
    "import seaborn as sns\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPool2D\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = util_mnist_reader.load_mnist('data/fashion', kind='train')\n",
    "X_test, y_test = util_mnist_reader.load_mnist('data/fashion', kind='t10k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y, n_labels):\n",
    "    mat = np.zeros((len(y), n_labels))\n",
    "    for i, val in enumerate(y):\n",
    "        mat[i, val] = 1\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X/255.0\n",
    "y = y.reshape(-1,1)\n",
    "X_test = X_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = one_hot(y,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nn_classifier():\n",
    "    def __init__(self, epochs, alpha, n_batches,hidden_layer_nodes):\n",
    "        self.epochs = epochs\n",
    "        self.alpha = alpha\n",
    "        self.n_batches = n_batches\n",
    "        self.hidden_layer_nodes = hidden_layer_nodes\n",
    "        self.w1, self.w2 = self.weight_init()\n",
    "        self.preds = []\n",
    "        self.error = []\n",
    "        self.losstrack = []\n",
    "        self.accuracy_track = []\n",
    "\n",
    "    def weight_init(self):\n",
    "        w1 = np.random.uniform(-1.0, 1.0,size=(self.hidden_layer_nodes, 784))        \n",
    "        w2 = np.random.uniform(-1.0, 1.0,size=(10, self.hidden_layer_nodes))\n",
    "        return w1,w2\n",
    "        \n",
    "    def score(self,preds,actuals):\n",
    "        return np.sum(actuals == preds, axis=0) / float(actuals.shape[0])\n",
    "    \n",
    "    def cross_entropy(self, y_target, outputs):\n",
    "#         print(y_target.shape,\"Y\")\n",
    "#         print(outputs.shape,\"out\")\n",
    "        return -np.sum(sp.log(outputs) * y_target, axis=1)\n",
    "    \n",
    "    def softmax(self,z):\n",
    "        sum = np.sum(np.exp(z.T), axis=1).reshape(-1,1)\n",
    "        return (np.exp(z.T) / sum).T\n",
    "    \n",
    "    def sigmoid(self,z):\n",
    "        return 1 / (1 + np.exp(-z.astype(float)))\n",
    "    \n",
    "    def sigmoid_prime(self,z):\n",
    "        a1 = self.sigmoid(z)\n",
    "        return a1 * (1 - a1)\n",
    "    \n",
    "    def get_argmax(self,y):\n",
    "        return np.argmax(y, axis=1)\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        z1 = self.w1.dot(X.T)\n",
    "        a1 = self.sigmoid(z1)\n",
    "        z2 = self.w2.dot(a1)\n",
    "        a2 = self.softmax(z2)\n",
    "        return z1,a1,z2,a2\n",
    "        \n",
    "    def error(self, y_actual, y_preds):\n",
    "        error = cross_entropy(y_actual, y_preds)\n",
    "        return 0.5 * np.mean(error)\n",
    "    \n",
    "    def backward_pass(self,X_train,y_train,a1,a2,z1):\n",
    "        intermediate_1 = a2 - y_train.T\n",
    "        intermediate_2 = self.w2.T.dot(intermediate_1) * self.sigmoid_prime(z1)\n",
    "        dw1 = intermediate_2.dot(X_train)\n",
    "        dw2 = intermediate_1.dot(a1.T)\n",
    "        self.w1 -= self.alpha * dw1\n",
    "        self.w2 -= self.alpha * dw2\n",
    "        \n",
    "    def predict(self,X):\n",
    "        z1 = self.w1.dot(X.T)\n",
    "        a1 = self.sigmoid(z1)\n",
    "        z2 = self.w2.dot(a1)\n",
    "        a2 = self.softmax(z2)\n",
    "        return self.get_argmax(a2.T)\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        X_train = X.copy()\n",
    "        y_train = y.copy()\n",
    "        X_batch = np.array_split(X_train, self.n_batches)\n",
    "        y_batch = np.array_split(y_train, self.n_batches)\n",
    "        for i in range(self.epochs):\n",
    "            preds = []\n",
    "            errors = []\n",
    "            for X_train,y_train in zip(X_batch, y_batch):\n",
    "                z1,a1,z2,a2 = self.forward_pass(X_train)\n",
    "                self.backward_pass(X_train,y_train,a1,a2,z1)\n",
    "                \n",
    "                error = self.cross_entropy(y_train,a2.T)\n",
    "                errors.append(0.5 * np.mean(error))\n",
    "        \n",
    "                y_preds = self.get_argmax(a2.T)\n",
    "                y_actual = self.get_argmax(y_train)\n",
    "                preds.append(self.score(y_preds,y_actual))\n",
    "                \n",
    "            self.losstrack.append(np.mean(errors))\n",
    "            self.accuracy_track.append(np.mean(preds))\n",
    "            \n",
    "    def plot_loss(self,):\n",
    "        plt.title(\"Loss vs. Epochs\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.plot(self.losstrack, label=\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    def plot_accuracy(self,):\n",
    "        plt.title(\"Accuracy vs. Epochs\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.plot(self.accuracy_track, label=\"Accuracy\")\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn_classifier(500,10e-4,15,300)\n",
    "model.fit(X,y)\n",
    "model.plot_loss()\n",
    "model.plot_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_preds = model.predict(X_test)\n",
    "test_accuracy = model.score(y_test_preds, y_test)\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_test_preds)\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_test, y_test_preds)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changed Learning rate to 10e-5 and Epochs to 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn_classifier(100,10e-5,15,300)\n",
    "model.fit(X,y)\n",
    "model.plot_loss()\n",
    "model.plot_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_preds = model.predict(X_test)\n",
    "test_accuracy = model.score(y_test_preds, y_test)\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_test_preds)\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_test, y_test_preds)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training using Keras Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = util_mnist_reader.load_mnist('data/fashion', kind='train')\n",
    "X_test, y_test = util_mnist_reader.load_mnist('data/fashion', kind='t10k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nn = Sequential()\n",
    "model_nn.add(Dense(units = 300, activation = 'sigmoid', input_dim = 784))\n",
    "model_nn.add(Dense(units = 200, activation = 'sigmoid'))\n",
    "model_nn.add(Dense(units = 75, activation = 'sigmoid'))\n",
    "model_nn.add(Dense(units = 10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nn.compile(loss='categorical_crossentropy',optimizer='sgd',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = model_nn.fit(X_train, one_hot(y_train, 10), epochs = 300, batch_size = 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_nn.evaluate(X_test, one_hot(y_test, 10), batch_size = 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_model.history['accuracy'])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"With Learning rate 0.01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_model.history['loss'])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"With Learning rate 0.01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_preds = model_nn.predict_classes(X_test)\n",
    "cm = confusion_matrix(y_test, y_test_preds)\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_test, y_test_preds)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nn = Sequential()\n",
    "model_nn.add(Dense(units = 200, activation = 'sigmoid', input_dim = 784))\n",
    "model_nn.add(Dense(units = 100, activation = 'sigmoid'))\n",
    "model_nn.add(Dense(units = 50, activation = 'sigmoid'))\n",
    "model_nn.add(Dense(units = 10, activation = 'softmax'))\n",
    "optimizers.sgd(learning_rate=10e-4)\n",
    "model_nn.compile(loss='categorical_crossentropy',optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = model_nn.fit(X_train, one_hot(y_train, 10), epochs = 300, batch_size = 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_nn.evaluate(X_test, one_hot(y_test, 10), batch_size = 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_model.history['accuracy'])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"With Learning rate 0.0001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_model.history['loss'])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"With Learning rate 0.0001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_preds = model_nn.predict_classes(X_test)\n",
    "cm = confusion_matrix(y_test, y_test_preds)\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_test, y_test_preds)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with Convolution Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = util_mnist_reader.load_mnist('data/fashion', kind='train')\n",
    "X_test, y_test = util_mnist_reader.load_mnist('data/fashion', kind='t10k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X/255.0\n",
    "y = y.reshape(-1,1)\n",
    "X_test = X_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X.reshape(60000,28,28,1)\n",
    "X_test = X_test.reshape(10000,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = one_hot(y_test,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn = Sequential()\n",
    "model_cnn.add(Conv2D(200, kernel_size = 3, activation = 'sigmoid', input_shape = (28, 28, 1)))\n",
    "model_cnn.add(MaxPool2D())\n",
    "model_cnn.add(Conv2D(300, kernel_size = 3, activation = 'sigmoid'))\n",
    "model_cnn.add(MaxPool2D())\n",
    "model_cnn.add(Flatten())\n",
    "model_cnn.add(Dense(units = 10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn.compile(loss='categorical_crossentropy',optimizer='sgd',metrics=['accuracy'])\n",
    "train_model = model_cnn.fit(X_train, one_hot(y_train, 10), epochs = 10, batch_size = 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss_acc = model.evaluate(X_test, one_hot(y_test, 10), batch_size = 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_model.history['acc'])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"With Learning rate 0.01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_model.history['loss'])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"With Learning rate 0.01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_preds = model.predict_classes(X_test)\n",
    "cm = confusion_matrix(y_test, test_preds)\n",
    "print(cm)\n",
    "report = classification_report(y_test, y_test_preds)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn = Sequential()\n",
    "model_cnn.add(Conv2D(100, kernel_size = 3, activation = 'sigmoid', input_shape = (28, 28, 1)))\n",
    "model_cnn.add(MaxPool2D())\n",
    "model_cnn.add(Conv2D(50, kernel_size = 3, activation = 'sigmoid'))\n",
    "model_cnn.add(MaxPool2D())\n",
    "model_cnn.add(Flatten())\n",
    "model_cnn.add(Dense(units = 10, activation = 'softmax'))\n",
    "optimizers.sgd(lr = 10e-4)\n",
    "model_cnn.compile(loss='categorical_crossentropy',optimizer='sgd',metrics=['accuracy'])\n",
    "train_model = model_cnn.fit(X_train, one_hot(y_train, 10), epochs = 200, batch_size = 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_model.history['acc'])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"With Learning rate 0.0001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_model.history['loss'])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"With Learning rate 0.0001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = model.predict_classes(X_test)\n",
    "cm = confusion_matrix(y_test, test_preds)\n",
    "print(cm)\n",
    "report = classification_report(y_test, test_preds)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
